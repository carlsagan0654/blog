---
title: "–°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ Ollama"
date: 2024-12-27T21:00:00+03:00
draft: false
tags: ["python", "jupyter", "bigdata"]
categories: ["–±–ª–æ–≥"]
toc: true
---

–° –ø–æ–º–æ—â—å—é chatgpt –Ω–∞–ø–∏—Å–∞–ª –ø—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –ò–ò –∫ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏–∫—Ä—É—Ç–∏—Ç—å –∫ –ª—é–±–æ–º—É –≥—Ä–∞–±–±–µ—Ä—É, —Ç–∞–∫ —á—Ç–æ –æ—Å—Ç–∞–≤–ª—é —Ç—É—Ç –≤–º–µ—Å—Ç–µ —Å –ø—Ä–∏–º–µ—Ä–æ–º; —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∞ –º–æ–¥–µ–ª—å mistral, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª—Å—è –∑–∞–ø—Ä–æ—Å "–¥–µ—Ç–µ–∫—Ç–∏–≤–∞" –∏–º–µ–Ω–Ω–æ –ø–æ–¥ –Ω–µ–µ.

```python
!pip install ollama
```

    Collecting ollama
      Downloading ollama-0.4.4-py3-none-any.whl.metadata (4.7 kB)
    Requirement already satisfied: httpx<0.28.0,>=0.27.0 in ./Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from ollama) (0.27.0)
    Collecting pydantic<3.0.0,>=2.9.0 (from ollama)
      Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)
    Requirement already satisfied: anyio in ./Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.4.0)
    Requirement already satisfied: certifi in ./Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.7.4)
    Requirement already satisfied: httpcore==1.* in ./Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)
    Requirement already satisfied: idna in ./Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.8)
    Requirement already satisfied: sniffio in ./Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)
    Requirement already satisfied: h11<0.15,>=0.13 in ./Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)
    Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.9.0->ollama)
      Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
    Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.9.0->ollama)
      Downloading pydantic_core-2.27.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)
    Requirement already satisfied: typing-extensions>=4.12.2 in ./Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)
    Downloading ollama-0.4.4-py3-none-any.whl (13 kB)
    Downloading pydantic-2.10.4-py3-none-any.whl (431 kB)
    Downloading pydantic_core-2.27.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)
    [2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m1.8/1.8 MB[0m [31m7.1 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
    Installing collected packages: pydantic-core, annotated-types, pydantic, ollama
    Successfully installed annotated-types-0.7.0 ollama-0.4.4 pydantic-2.10.4 pydantic-core-2.27.2



```python
from ollama import chat
from ollama import ChatResponse
import pickle

class OllamaChat:
    def __init__(self, model_name='mistral', load_file: str = None):
        if load_file:
            self.load_state(load_file)
        else:
            self.model_name = model_name
            self.history = []

    def create_system_prompt(self, system_message: str):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–æ –∏—Å—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ –µ–≥–æ –µ—â—ë –Ω–µ—Ç."""
        if not any(msg['role'] == 'system' for msg in self.history):
            self.history.insert(0, {
                'role': 'system',
                'content': system_message,
            })

    def add_pseudo_interaction(self, pseudo_user_message: str, pseudo_response: str):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –ø—Å–µ–≤–¥–æ-–∑–∞–ø—Ä–æ—Å –∏ –ø—Å–µ–≤–¥–æ-–æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é."""
        if pseudo_user_message and pseudo_response:
            self.history.append({
                'role': 'user',
                'content': pseudo_user_message.strip(),
            })
            self.history.append({
                'role': 'assistant',
                'content': pseudo_response.strip(),
            })

    def send_message(self, user_message: str) -> str:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        if not user_message.strip():
            raise ValueError("User message cannot be empty or whitespace.")

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.history.append({
            'role': 'user',
            'content': user_message.strip(),
        })

        try:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—å
            response: ChatResponse = chat(model=self.model_name, messages=self.history)
            model_response = response.message.content.strip()
        except Exception as e:
            raise RuntimeError(f"Failed to get a response from the model: {e}")

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.history.append({
            'role': 'assistant',
            'content': model_response,
        })

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        return model_response

    def replace_last_interaction(self, new_user_message: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç, –æ–±—Ä–∞—â–∞—è—Å—å –∫ –º–æ–¥–µ–ª–∏."""
        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if len(self.history) >= 2 and self.history[-1]['role'] == 'assistant' and self.history[-2]['role'] == 'user':
            self.history.pop()  # –£–¥–∞–ª—è–µ–º –æ—Ç–≤–µ—Ç
            self.history.pop()  # –£–¥–∞–ª—è–µ–º –∑–∞–ø—Ä–æ—Å

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
        return self.send_message(new_user_message)

    def remove_last_message(self):
        """–£–¥–∞–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏."""
        if self.history:
            self.history.pop()
        else:
            raise ValueError("History is empty; nothing to remove.")

    def save_state(self, file_path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –≤ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª."""
        try:
            with open(file_path, 'wb') as file:
                pickle.dump({'model_name': self.model_name, 'history': self.history}, file)
        except Exception as e:
            raise IOError(f"Failed to save state to {file_path}: {e}")

    def load_state(self, file_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –∏–∑ –±–∏–Ω–∞—Ä–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""
        try:
            with open(file_path, 'rb') as file:
                state = pickle.load(file)
                self.model_name = state['model_name']
                self.history = state['history']
        except Exception as e:
            raise IOError(f"Failed to load state from {file_path}: {e}")

    def clear_history_except_system(self):
        """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞."""
        if any(msg['role'] == 'system' for msg in self.history):
            self.history = [message for message in self.history if message['role'] == 'system']
        else:
            raise ValueError("No system prompt found to preserve.")

    def add_contextual_prompt(self, context_message: str):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏."""
        self.history.append({
            'role': 'assistant',
            'content': context_message.strip(),
        })

    def remove_contextual_prompts(self):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏."""
        self.history = [message for message in self.history if message['role'] != 'context']

    def get_history_summary(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."""
        return "\n".join([f"[{msg['role']}] {msg['content']}" for msg in self.history])

```

```python
ollama_chat = OllamaChat()
ollama_chat.create_system_prompt("""You are a highly skilled text analysis expert, acting as a meticulous and unbiased detective. 
Your objective is to analyze written text or correspondence to extract only explicitly stated sensitive and confidential information, such as:
1) IP addresses
2) Usernames
3) Passwords
4) Access keys
5) Server details
6) Financial data (e.g., bank account numbers, credit card numbers, CVVs)
8) Any other explicitly mentioned confidential or critical information

Rules:
Extract Only Explicit Data: Extract only information explicitly mentioned in the input. If the input does not contain sensitive data, output only <OMITTED> as a single word with no additional text or commentary.
Strict Output Format:
If sensitive data is found, format it strictly as:
IP: <IP_ADDRESS>; User: <USERNAME>; Password: <PASSWORD>; Server: <SERVER_DETAILS>; Access Key: <ACCESS_KEY>; Financial Data: <FINANCIAL_DETAILS>; time: <DATE_TIME>
Separate multiple entries with line breaks.
If no data is present, output only <OMITTED> (no other words, explanations, or comments).
Ignore Irrelevant Data: Ignore all non-sensitive information, including installation commands, general descriptions, and other unrelated text.
No Fabrication: Do not infer, guess, or fabricate data. Extract only what is explicitly mentioned.

Examples:
Input 1:
IP: 192.168.1.2; User: john_doe; Password: secret1234; Server: server01.example.com; time: 2022-01-01 10:30
Output 1:
IP: 192.168.1.2; User: john_doe; Password: secret1234; Server: server01.example.com; time: 2022-01-01 10:30

Input 2:
We recommend using this model with the vLLM library to implement production-ready inference pipelines. Installation: pip install --upgrade vllm
Output 2:
<OMITTED>

Input 3:
Access Key: ABCDEFGHIJKLMNOPQRST; Server: example.com; time: 2023-12-21 18:30
Output 3:
Access Key: ABCDEFGHIJKLMNOPQRST; Server: example.com; time: 2023-12-21 18:30

Input 4:
No sensitive information provided here.
Output 4:
<OMITTED>
""")

try:
    resp = ollama_chat.send_message("""
    We recommend using this model with the vLLM library to implement production-ready inference pipelines.
    Installation
    Make sure you install vLLM >= v0.6.1.post1:
    pip install --upgrade vllm
    Also make sure you have mistral_common >= 1.4.1 installed:
    pip install --upgrade mistral_common
    Dear customer
    Dear customer,

    The Server 4 RAM / 2 CPU / 80 NVMe .hosted-by-vdsina.com service has been reinstalled on your account.
    
    Access data
    
    OS Ubuntu 24.04
    
    IP 111.11.138.111
    User root
    Password 4iE1111111Q35Be

    """)
    print(f"'{resp}'")
except Exception as e:
    print(f"An error occurred: {e}")
```
    'IP: 111.11.138.111; User: root; Password: 4iE1111111Q35Be; time: <TIME>'